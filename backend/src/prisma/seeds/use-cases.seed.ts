import { PrismaClient, UseCaseStatus, UseCaseCategory, UseCaseIndustry, UseCaseImplementationType } from '@prisma/client';

/**
 * Seed Use Cases
 *
 * Creates 5 comprehensive, real-world LLM use cases
 */

interface UseCaseSeedData {
  title: string;
  slug: string;
  summary: string;
  problem: string;
  solution: string;
  implementation: string;
  results: string;
  category: UseCaseCategory;
  industry: UseCaseIndustry;
  implementationType: UseCaseImplementationType[];
  models: string[]; // model slugs
  companyName: string;
  companySize: string;
  authorUsername: string;
}

const useCases: UseCaseSeedData[] = [
  {
    title: 'AI-Powered Customer Support System Reduces Response Time by 80%',
    slug: 'ai-customer-support-reduces-response-time',
    summary: 'A SaaS company implemented a RAG-based customer support system using GPT-4, reducing average response time from 2 hours to 24 minutes while maintaining 94% customer satisfaction.',
    problem: `Our SaaS platform had a growing customer base (10,000+ users) but our support team of 5 agents was overwhelmed. Key challenges:

- **Response time**: Average 2 hours during business hours, 8+ hours overnight
- **Repetitive questions**: 60% of tickets were about the same 20 topics
- **Knowledge base**: 500+ articles but users rarely found them
- **Scaling costs**: Each new 2000 customers required hiring another agent
- **Burnout**: Support team was stretched thin, affecting quality`,
    solution: `We implemented a two-tier RAG-based support system:

## Tier 1: Automated First Response
- RAG system retrieves relevant docs from our knowledge base
- GPT-4 generates personalized response
- If confidence > 85%, response sent automatically
- If confidence < 85%, escalated to human with draft response

## Tier 2: Human + AI
- Agent receives ticket with AI-generated draft
- Agent can edit/approve/rewrite
- Agent has context from retrieved docs
- System learns from agent edits

## Technical Implementation
- **Embeddings**: OpenAI ada-002 for documentation
- **Vector DB**: Weaviate (self-hosted)
- **LLM**: GPT-4 for generation, GPT-3.5 for classification
- **Monitoring**: Custom dashboard tracking confidence scores and overrides`,
    implementation: `### 1. Document Processing
\`\`\`python
# Chunk documentation with semantic splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\\n\\n", "\\n", ". "]
)

chunks = splitter.split_documents(docs)

# Generate embeddings and store in Weaviate
embeddings = OpenAIEmbeddings()
vectorstore = Weaviate.from_documents(
    chunks,
    embeddings,
    weaviate_url=WEAVIATE_URL,
    by_text=False
)
\`\`\`

### 2. Query Processing
\`\`\`python
def process_support_ticket(ticket_text: str) -> dict:
    # Retrieve relevant docs
    docs = vectorstore.similarity_search(
        ticket_text,
        k=5
    )

    # Rerank with Cohere
    reranked = cohere_client.rerank(
        query=ticket_text,
        documents=[d.page_content for d in docs],
        top_n=3,
        model="rerank-english-v2.0"
    )

    # Generate response with GPT-4
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": SUPPORT_SYSTEM_PROMPT},
            {"role": "user", "content": f"Context: {reranked}\\n\\nTicket: {ticket_text}"}
        ],
        temperature=0.3
    )

    # Calculate confidence score
    confidence = calculate_confidence(response, reranked)

    return {
        "response": response.choices[0].message.content,
        "confidence": confidence,
        "sources": reranked,
        "auto_send": confidence > 0.85
    }
\`\`\`

### 3. Learning Loop
\`\`\`python
# Track agent edits to improve system
def log_agent_edit(original_response, edited_response, ticket):
    # Store for fine-tuning data
    training_examples.append({
        "ticket": ticket,
        "good_response": edited_response,
        "context": relevant_docs
    })

    # If > 100 examples, trigger fine-tuning job
    if len(training_examples) > 100:
        trigger_finetuning()
\`\`\``,
    results: `### Quantitative Results (6 months)
- **Response time**: 2 hours ‚Üí 24 minutes (80% reduction)
- **First contact resolution**: 45% ‚Üí 72%
- **Support costs**: $15,000/mo ‚Üí $8,000/mo (47% reduction)
- **Customer satisfaction**: 87% ‚Üí 94%
- **Tickets handled/agent**: 50/day ‚Üí 120/day

### Qualitative Results
- Support team morale improved (less repetitive work)
- Agents became "AI supervisors" rather than responders
- Knowledge base quality improved (identified gaps)
- 24/7 support without hiring night shift
- Consistent response quality across all agents

### Financial Impact
- **Avoided hires**: 3 agents @ $60k = $180k/year
- **API costs**: ~$2,000/month (GPT-4 + embeddings)
- **ROI**: 600% in first year
- **Payback period**: 2 months`,
    category: 'customer_support',
    industry: 'saas',
    implementationType: ['rag', 'prompt_engineering', 'embeddings'],
    models: ['gpt-4', 'gpt-3-5-turbo'],
    companyName: 'TechFlow Inc.',
    companySize: '50-200 employees',
    authorUsername: 'maya_enterprise',
  },
  {
    title: 'Automated Code Review System Catches 40% More Issues Than Human Review Alone',
    slug: 'automated-code-review-system',
    summary: 'Engineering team implemented Claude 3.5 Sonnet for automated code reviews, improving code quality metrics by 35% and reducing security vulnerabilities by 60%.',
    problem: `Our engineering team of 25 developers was growing fast, but code quality was suffering:

- **Inconsistent reviews**: Different reviewers had different standards
- **Security gaps**: Only 30% of PRs got security review
- **Slow reviews**: Average 2 days to get review feedback
- **Burnout**: Senior devs spending 6+ hours/week on reviews
- **Knowledge silos**: Only 3 devs understood critical parts of codebase`,
    solution: `We built an AI-powered code review system using Claude 3.5 Sonnet that runs automatically on every PR:

## Review Levels
1. **Automated checks** (runs first)
   - Code style and conventions
   - Common bugs and anti-patterns
   - Performance issues
   - Security vulnerabilities

2. **AI deep review** (if automated checks pass)
   - Architecture analysis
   - Edge case identification
   - Test coverage assessment
   - Documentation quality

3. **Human review** (with AI insights)
   - Reviewer sees AI findings
   - Focuses on business logic and design
   - Can override AI suggestions

## Key Features
- Integrated with GitHub PR workflow
- Comments directly on code lines
- Links to documentation and examples
- Learns from human reviewer feedback`,
    implementation: `### 1. PR Hook Integration
\`\`\`python
@github_webhook('/pr')
async def handle_pull_request(pr_data):
    if pr_data['action'] not in ['opened', 'synchronize']:
        return

    # Get changed files
    files = await github.get_pr_files(pr_data['pr_number'])

    # Run AI review
    review_results = await review_with_claude(files)

    # Post comments
    await github.post_review_comments(
        pr_data['pr_number'],
        review_results
    )
\`\`\`

### 2. Claude Code Review
\`\`\`python
async def review_with_claude(files):
    reviews = []

    for file in files:
        # Get file context (imports, related files)
        context = await get_file_context(file)

        prompt = f"""Review this code change for:
1. Bugs and logic errors
2. Security vulnerabilities
3. Performance issues
4. Best practices violations
5. Edge cases not handled

File: {file.name}
Before:
{file.old_content}

After:
{file.new_content}

Context:
{context}

Provide specific, actionable feedback with line numbers."""

        response = await anthropic.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )

        reviews.append(parse_review_response(response))

    return reviews
\`\`\`

### 3. Learning from Human Feedback
\`\`\`python
# Track when humans override AI suggestions
@github_webhook('/review_comment')
async def track_human_feedback(comment_data):
    if comment_data['comment'].startswith('AI:'):
        # This is an AI suggestion
        human_action = comment_data['resolved_by_human']

        await log_feedback({
            'ai_suggestion': comment_data['comment'],
            'human_action': human_action,
            'code_snippet': comment_data['code'],
            'timestamp': datetime.now()
        })

        # Weekly: Update system prompt with learnings
        if feedback_count % 100 == 0:
            await update_review_guidelines()
\`\`\``,
    results: `### Quantitative Results (3 months)
- **Issues caught**: 40% more than human-only review
- **Security vulns**: 60% reduction in production
- **Review time**: 2 days ‚Üí 4 hours average
- **Code quality score**: +35% (via static analysis)
- **Senior dev time saved**: 150 hours/month

### Issue Breakdown
- **Bugs found by AI**: 156
- **Security issues**: 23 (including 3 critical)
- **Performance issues**: 67
- **Best practice violations**: 340+

### Developer Feedback
- 85% of devs prefer AI-assisted review
- "Catches things I always miss" - Senior Dev
- "Like having a senior dev review every PR" - Junior Dev
- "Freed me to focus on architecture" - Team Lead

### False Positive Rate
- **Initial**: 35% false positives
- **After tuning**: 12% false positives
- **Human override rate**: 8%`,
    category: 'code_generation',
    industry: 'saas',
    implementationType: ['prompt_engineering', 'agent'],
    models: ['claude-3-5-sonnet'],
    companyName: 'DevTools Corp',
    companySize: '20-50 employees',
    authorUsername: 'thomas_oss',
  },
  {
    title: 'E-commerce Product Description Generator Increases Conversion by 23%',
    slug: 'ecommerce-product-description-generator',
    summary: 'Online retailer automated product description generation using GPT-4, creating SEO-optimized, persuasive descriptions that increased conversion rates by 23% and saved 40 hours/week.',
    problem: `Our e-commerce store had 15,000+ products but inconsistent descriptions:

- **Manual effort**: 2 copywriters spending full-time writing descriptions
- **Inconsistent quality**: Descriptions varied wildly in tone and completeness
- **Poor SEO**: Missing keywords, not optimized for search
- **Slow updates**: Took weeks to update seasonal product descriptions
- **Scaling issue**: Adding 500 new products/month but could only write 300 descriptions`,
    solution: `Implemented a GPT-4 based system that generates high-quality, SEO-optimized product descriptions automatically:

## System Components
1. **Data extraction**: Pull product specs from supplier feeds
2. **Competitor analysis**: Analyze top-performing listings
3. **Description generation**: GPT-4 creates multiple variations
4. **SEO optimization**: Ensure keyword inclusion and structure
5. **Quality scoring**: Rate descriptions on multiple factors
6. **Human review**: Copywriters review flagged items only

## Customization Levels
- **Brand voice**: Consistent tone across all descriptions
- **Category-specific**: Different templates for clothing vs electronics
- **Seasonal**: Automatic updates for holidays/seasons
- **A/B testing**: Generate variations for testing`,
    implementation: `### 1. Product Data Extraction
\`\`\`python
def extract_product_features(product_id):
    # Get product data from database
    product = db.get_product(product_id)

    # Get competitor data
    competitors = scrape_competitor_listings(
        product.category,
        product.name
    )

    # Get top keywords from search data
    keywords = analytics.get_top_keywords(product.category)

    return {
        'specs': product.specs,
        'category': product.category,
        'price_point': product.price,
        'top_keywords': keywords[:10],
        'competitor_features': extract_common_features(competitors)
    }
\`\`\`

### 2. Description Generation
\`\`\`python
def generate_product_description(product_data):
    prompt = f"""Generate a compelling product description for:

Product: {product_data['name']}
Category: {product_data['category']}
Key Features: {product_data['specs']}
Price Point: ${product_data['price']}

Requirements:
- Length: 150-200 words
- Tone: {BRAND_VOICE}
- Include keywords: {product_data['top_keywords']}
- Highlight unique features vs competitors
- Include emotional appeal
- SEO optimized
- Clear call-to-action

Format:
- Main description (2-3 paragraphs)
- Bullet points (5-7 key features)
- Meta description (150 chars)"""

    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": COPYWRITER_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )

    return parse_description(response.choices[0].message.content)
\`\`\`

### 3. Quality Scoring
\`\`\`python
def score_description(description, product_data):
    scores = {
        'keyword_density': check_keyword_usage(description, product_data['top_keywords']),
        'readability': calculate_flesch_score(description),
        'length': check_length_requirements(description),
        'tone_consistency': check_brand_voice(description),
        'uniqueness': check_plagiarism(description)
    }

    overall_score = sum(scores.values()) / len(scores)

    # Flag for human review if score < 0.8
    return overall_score, scores
\`\`\``,
    results: `### Business Results (6 months)
- **Conversion rate**: 3.2% ‚Üí 3.9% (+23%)
- **Time saved**: 40 hours/week copywriting time
- **Descriptions created**: 12,000+ generated
- **SEO ranking**: +45% improvement average
- **Revenue impact**: +$180,000/month

### Quality Metrics
- **Human approval rate**: 92% (8% edited)
- **Customer feedback**: No negative comments about descriptions
- **Bounce rate**: -15% (better descriptions = better match)
- **Time on page**: +25% (more engaging content)

### Cost Analysis
- **Previous cost**: 2 copywriters @ $50k = $100k/year
- **New cost**: 1 copywriter + $3k API = $53k/year
- **Savings**: $47k/year
- **ROI**: 900% (including revenue increase)

### Copywriter Feedback
- "I went from writing to editing and strategy"
- "Can now focus on homepage and category pages"
- "AI handles boring specs, I do creative work"`,
    category: 'content_creation',
    industry: 'ecommerce',
    implementationType: ['prompt_engineering', 'function_calling'],
    models: ['gpt-4'],
    companyName: 'StyleHub Marketplace',
    companySize: '10-50 employees',
    authorUsername: 'lisa_founder',
  },
  {
    title: 'Financial Document Analysis System Reduces Processing Time by 90%',
    slug: 'financial-document-analysis-system',
    summary: 'Investment firm automated financial document analysis using GPT-4 and Claude, processing 1000+ page reports in minutes instead of days while improving accuracy.',
    problem: `Our investment research team needed to analyze hundreds of financial documents:

- **Manual review**: 2-3 days per quarterly report
- **Volume**: 200+ companies to track
- **Inconsistency**: Different analysts focus on different metrics
- **Fatigue**: Analysts miss important details in 100+ page docs
- **Time-sensitive**: Market moves on news, need fast insights`,
    solution: `Built a multi-model system that analyzes financial documents and extracts key insights:

## Processing Pipeline
1. **Document parsing**: Extract text, tables, and charts
2. **Section identification**: Find MD&A, financial statements, risks
3. **Data extraction**: Pull key metrics and figures
4. **Sentiment analysis**: Assess tone and confidence
5. **Comparative analysis**: Compare to previous quarters and competitors
6. **Summary generation**: Create executive summary
7. **Flag generation**: Highlight risks and opportunities

## Model Selection
- **Claude 3 Opus**: Long documents (200K context for full reports)
- **GPT-4**: Complex reasoning and comparisons
- **GPT-3.5**: Fast classification and extraction`,
    implementation: `### 1. Document Processing
\`\`\`python
async def process_financial_document(document_path):
    # Parse PDF with tables
    text, tables = extract_document_content(document_path)

    # Use Claude for initial analysis (can fit entire doc)
    initial_analysis = await anthropic.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=4000,
        messages=[{
            "role": "user",
            "content": f"""Analyze this financial document and extract:
1. Key financial metrics (revenue, profit, margins, etc.)
2. Forward guidance
3. Risk factors
4. Management sentiment
5. Major changes vs previous quarter

Document:
{text}

Tables:
{tables}

Format response as structured JSON."""
        }]
    )

    return parse_analysis(initial_analysis)
\`\`\`

### 2. Comparative Analysis
\`\`\`python
async def compare_quarters(current, previous):
    prompt = f"""Compare these financial results:

Current Quarter:
{json.dumps(current, indent=2)}

Previous Quarter:
{json.dumps(previous, indent=2)}

Provide:
1. Key changes (% and absolute)
2. Trend analysis
3. Areas of concern
4. Positive developments
5. Investment implications"""

    response = await openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a financial analyst"},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content
\`\`\`

### 3. Risk Detection
\`\`\`python
def detect_red_flags(analysis):
    # Use GPT-4 to identify concerning patterns
    red_flags = openai.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "system",
            "content": "You are a risk analyst. Identify red flags in financial reports."
        }, {
            "role": "user",
            "content": f"Analyze for risks: {analysis}"
        }]
    )

    return parse_red_flags(red_flags.choices[0].message.content)
\`\`\``,
    results: `### Time Savings
- **Before**: 2-3 days per report √ó 200 reports = 400-600 analyst-days/quarter
- **After**: 15 minutes per report √ó 200 reports = 50 analyst-hours/quarter
- **Time saved**: 90%+ reduction in processing time

### Accuracy Improvements
- **Data extraction**: 99.2% accuracy (vs 97.8% manual)
- **Missed insights**: 45% fewer important details missed
- **Consistency**: 100% of reports get same depth of analysis

### Business Impact
- **Faster decisions**: Insights within hours instead of days
- **Competitive advantage**: React to market news faster
- **Analyst satisfaction**: Focus on strategy not data entry
- **Coverage**: Expanded from 200 to 500 companies with same team

### Cost Analysis
- **API costs**: $5,000/month (Claude + GPT-4)
- **Analyst time saved**: 600 hours/quarter @ $100/hr = $60,000
- **ROI**: 1100% quarterly
- **Quality improvement**: Priceless`,
    category: 'data_analysis',
    industry: 'finance',
    implementationType: ['rag', 'prompt_engineering', 'extraction'],
    models: ['claude-3-opus', 'gpt-4', 'gpt-3-5-turbo'],
    companyName: 'Apex Investments',
    companySize: '100-500 employees',
    authorUsername: 'maya_enterprise',
  },
  {
    title: 'Personalized Learning Platform Using LLMs Improves Student Outcomes by 40%',
    slug: 'personalized-learning-platform-llm',
    summary: 'EdTech startup built an AI tutor using GPT-4 that adapts to each student\'s learning style, resulting in 40% faster concept mastery and 85% student satisfaction.',
    problem: `Our online learning platform had generic content that didn't adapt to individual students:

- **One-size-fits-all**: Same explanations for all learning styles
- **No feedback loop**: Students stuck without personalized help
- **Teacher shortage**: 1 teacher per 50 students
- **Dropout rate**: 35% of students quit due to frustration
- **Engagement**: Average session only 12 minutes`,
    solution: `Created an AI-powered personalized tutor using GPT-4:

## Key Features
1. **Adaptive explanations**: Different teaching styles based on student
2. **Real-time help**: Answers questions 24/7
3. **Socratic method**: Guides students to answers vs giving them
4. **Progress tracking**: Identifies knowledge gaps
5. **Engagement techniques**: Makes learning fun and interactive

## Personalization Factors
- Learning style (visual, auditory, kinesthetic)
- Current knowledge level
- Pace preference
- Areas of difficulty
- Interest areas for examples`,
    implementation: `### 1. Student Profiling
\`\`\`python
def build_student_profile(student_id):
    # Analyze historical data
    history = db.get_student_history(student_id)

    profile = {
        'learning_style': infer_learning_style(history),
        'knowledge_level': assess_current_level(history),
        'struggle_areas': identify_struggles(history),
        'interests': extract_interests(history),
        'optimal_pace': calculate_pace(history)
    }

    return profile
\`\`\`

### 2. Adaptive Teaching
\`\`\`python
def generate_explanation(concept, student_profile):
    system_prompt = f"""You are an AI tutor. Adapt your teaching style to:
- Learning style: {student_profile['learning_style']}
- Current level: {student_profile['knowledge_level']}
- Interests: {student_profile['interests']}

Use the Socratic method - guide with questions, don't just give answers.
Use examples from their interests when possible.
Break complex ideas into simple steps."""

    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Explain: {concept}"}
        ],
        temperature=0.7
    )

    return response.choices[0].message.content
\`\`\`

### 3. Progress Assessment
\`\`\`python
async def assess_understanding(conversation_history):
    prompt = f"""Based on this tutoring conversation, assess:
1. Student's understanding level (0-100)
2. Remaining knowledge gaps
3. Recommended next topics
4. Learning velocity

Conversation:
{conversation_history}"""

    assessment = await openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return parse_assessment(assessment.choices[0].message.content)
\`\`\``,
    results: `### Learning Outcomes (6 months)
- **Concept mastery time**: 40% faster
- **Test scores**: +18% average improvement
- **Completion rate**: 65% ‚Üí 82%
- **Engagement**: Session time 12 min ‚Üí 28 min

### Student Satisfaction
- **Overall satisfaction**: 85%
- **Prefer AI tutor**: 72% (vs generic content)
- **"Feels like 1-on-1 tutoring"**: 68%
- **Would recommend**: 89%

### Qualitative Feedback
- "Explains things the way I understand"
- "Never feel stupid asking questions"
- "Like having a patient teacher 24/7"
- "Makes learning actually fun"

### Business Metrics
- **Reduced churn**: 35% ‚Üí 18%
- **Increased LTV**: +$240 per student
- **Referrals**: +45% (students telling friends)
- **Revenue**: +65% growth

### Cost vs Value
- **API costs**: $15,000/month for 5,000 students
- **Cost per student**: $3/month
- **Value created**: Priceless education quality
- **Competitive advantage**: Unique differentiation`,
    category: 'automation',
    industry: 'education',
    implementationType: ['prompt_engineering', 'agent', 'function_calling'],
    models: ['gpt-4'],
    companyName: 'LearnSmart AI',
    companySize: '10-20 employees',
    authorUsername: 'emily_ai',
  },
];

export async function seedUseCases(prisma: PrismaClient): Promise<void> {
  console.log('üå± Seeding use cases...');

  let createdCount = 0;
  let updatedCount = 0;

  for (const useCaseData of useCases) {
    try {
      // Get author
      const author = await prisma.user.findFirst({
        where: { username: useCaseData.authorUsername },
      });

      if (!author) {
        console.log(`‚ö†Ô∏è  User ${useCaseData.authorUsername} not found, skipping use case`);
        continue;
      }

      // Check if use case exists
      const existingUseCase = await prisma.useCase.findFirst({
        where: { slug: useCaseData.slug },
      });

      let useCase;
      if (existingUseCase) {
        useCase = await prisma.useCase.update({
          where: { id: existingUseCase.id },
          data: {
            title: useCaseData.title,
            summary: useCaseData.summary,
            problem: useCaseData.problem,
            solution: useCaseData.solution,
            implementation: useCaseData.implementation,
            results: useCaseData.results,
            category: useCaseData.category,
            industry: useCaseData.industry,
            implementationType: useCaseData.implementationType,
            companyName: useCaseData.companyName,
            companySize: useCaseData.companySize,
            status: 'published',
          },
        });
        updatedCount++;
        console.log(`  ‚úÖ Updated use case: ${useCaseData.title}`);
      } else {
        useCase = await prisma.useCase.create({
          data: {
            title: useCaseData.title,
            slug: useCaseData.slug,
            summary: useCaseData.summary,
            problem: useCaseData.problem,
            solution: useCaseData.solution,
            implementation: useCaseData.implementation,
            results: useCaseData.results,
            category: useCaseData.category,
            industry: useCaseData.industry,
            implementationType: useCaseData.implementationType,
            companyName: useCaseData.companyName,
            companySize: useCaseData.companySize,
            authorId: author.id,
            status: 'published',
            viewCount: Math.floor(Math.random() * 2000) + 100,
          },
        });
        createdCount++;
        console.log(`  ‚úÖ Created use case: ${useCaseData.title}`);
      }

      // Link models
      for (const modelSlug of useCaseData.models) {
        const model = await prisma.lLMModel.findUnique({
          where: { slug: modelSlug },
        });

        if (model) {
          await prisma.useCaseModel.upsert({
            where: {
              useCaseId_modelId: {
                useCaseId: useCase.id,
                modelId: model.id,
              },
            },
            update: {},
            create: {
              useCaseId: useCase.id,
              modelId: model.id,
            },
          });
        }
      }
    } catch (error) {
      console.error(`‚ùå Error processing use case ${useCaseData.title}:`, error);
    }
  }

  console.log('\n‚úÖ Use cases seeding complete!');
  console.log(`   Created: ${createdCount}`);
  console.log(`   Updated: ${updatedCount}`);
}

export default seedUseCases;
